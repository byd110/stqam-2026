\documentclass[11pt]{article}
\usepackage{enumitem}
\usepackage{listings}
\usepackage[listings]{tcolorbox}
\usepackage{tikz}
\usepackage{url}

%\usepackage{algorithm2e}
\usetikzlibrary{arrows,automata,shapes}
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=2em]
\tikzstyle{bt} = [rectangle, draw, fill=blue!20, 
    text width=4em, text centered, rounded corners, minimum height=2em]

\lstset{ %
language=Java,
basicstyle=\ttfamily,commentstyle=\scriptsize\itshape,showstringspaces=false,breaklines=true,numbers=left}
\newtcbinputlisting{\codelisting}[3][]{
    extrude left by=1em,
    extrude right by=2em,
    listing file={#3},
    fonttitle=\bfseries,
    listing options={basicstyle=\ttfamily\footnotesize,numbers=left,language=Java,#1},
    listing only,
    hbox,
}
\lstdefinelanguage{JavaScript}{
  keywords={typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, 
do, else, case, break},
  keywordstyle=\color{blue}\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\color{red}\ttfamily,
  morestring=[b]',
  morestring=[b]''
}


\newtheorem{defn}{Definition}
\newtheorem{crit}{Criterion}

\newcommand{\handout}[5]{
  \noindent
  \begin{center}
  \framebox{
    \vbox{
      \hbox to 5.78in { {\bf Software Testing, Quality Assurance and Maintenance } \hfill #2 }
      \vspace{4mm}
      \hbox to 5.78in { {\Large \hfill #5  \hfill} }
      \vspace{2mm}
      \hbox to 5.78in { {\em #3 \hfill #4} }
    }
  }
  \end{center}
  \vspace*{4mm}
}

\newcommand{\lecture}[4]{\handout{#1}{#2}{#3}{#4}{Lecture #1}}
\topmargin 0pt
\advance \topmargin by -\headheight
\advance \topmargin by -\headsep
\textheight 8.9in
\oddsidemargin 0pt
\evensidemargin \oddsidemargin
\marginparwidth 0.5in
\textwidth 6.5in

\parindent 0in
\parskip 1.5ex
%\renewcommand{\baselinestretch}{1.25}

\begin{document}

\lecture{3 --- January 12, 2026}{Winter 2026}{Patrick Lam}{version 2}

\subsection*{Assertions}
We build tests using assertions. 
In JUnit, there are three basic built-in choices:
\begin{enumerate}[noitemsep]
\item assertTrue(aBooleanExpression)
\item assertEquals(expected, actual)
\item assertEquals(expected, actual, tolerance)
\end{enumerate}
(There are others too, but let's start with these.)

{\tt assertTrue} is more flexible, since you can
write anything with a boolean value. However, it can give
hard-to-diagnose error messages---you need try harder when using it
if you want good tests.

\paragraph{Using Assertions.}
Why use assertions? Assertions are good for:
\begin{itemize}[noitemsep]
\item checking all the things that should be true (more = better);
\item serving as documentation:
    when system in state $S_1$,
    and I do $X$,
    assert that the result should be $R$, and
    that system should be in $S_2$.
\item allowing failure diagnosis (include assertion messages!)
\end{itemize}

\newpage
There are alternatives to using assertions.
For instance, one can also do external result verification:
\begin{itemize}[noitemsep]
\item write output to files; and
\item use diff (or your own custom diff) to compare
  expected and actual output.
\end{itemize}

The twist is that the expected result is then not visible when looking
at test's source code. (What's a good workaround?)

\paragraph{Verifying Behaviour.} The key is to observe actions
(calls) of the SUT. Some options:
\begin{itemize}[noitemsep]
\item procedural behaviour verification
  (the challenge in that case: recording and verifying behaviour); or
\item expected behaviour specification
  (capturing the outbound calls of the SUT).
\end{itemize}

\section*{Representation invariants}
We said that assertions are good for checking all the things that should be
true. In particular, they can help with checking the integrity of complex data
structures; in such a case, we can aim to write out all the things that should be true of the data structure.
Let's look at examples from the Fuzzing chapter of the Fuzzing Book\footnote{\url{https://www.fuzzingbook.org/html/Fuzzer.html}}. The example isn't really a complex data structure, but it does have some expected properties.
Your typical CS341 data structure is going to have more complex properties, but they are harder to illustrate.

\begin{lstlisting}[language=Python]
airport_codes: dict[str, str] = {
    "YVR": "Vancouver",
    "JFK": "New York-JFK",
    "CDG": "Paris-Charles de Gaulle",
    "CAI": "Cairo",
    "LED": "St. Petersburg",
    "PEK": "Beijing",
    "HND": "Tokyo-Haneda",
    "AKL": "Auckland"
}  # plus many more
\end{lstlisting}
We can introduce a \emph{representation invariant} for this \texttt{dict}:
\begin{lstlisting}[language=Python]
def code_repOK(code: str) -> bool:
    assert len(code) == 3, "Airport code must have three characters: " + repr(code)
    for c in code:
        assert c.isalpha(), "Non-letter in airport code: " + repr(code)
        assert c.isupper(), "Lowercase letter in airport code: " + repr(code)
    return True
\end{lstlisting}
and we can check it:
\begin{lstlisting}[language=Python]
assert code_repOK("SEA")
\end{lstlisting}
which quietly does not fail. You can also invoke \texttt{code\_repOK} on all of the codes
in \texttt{airport\_codes}, and in fact, you can make a function to do so:
\begin{lstlisting}[language=Python]
def airport_codes_repOK():
  for code in airport_codes:
      assert code_repOK(code)
  return True
\end{lstlisting}
If this is an abstract data type, you would usually use a function to add to it:
\begin{lstlisting}[language=Python]
def add_new_airport(code: str, city: str) -> None:
    assert code_repOK(code)
    assert airport_codes_repOK()
    airport_codes[code] = city
    assert airport_codes_repOK()
\end{lstlisting}
which checks the data structure before-and-after as well as the input. Might be slow.

Note that this checks properties that are \emph{specific to your data structure}.
I call them domain-specific properties. It's not just ``program doesn't crash'', it's
something that encodes information about how your program is supposed to behave.
It's a matter of taste to encode the right things.

Here's a collection of checks for a red-black tree:
\begin{lstlisting}[language=Python]
class RedBlackTree:
  def repOK(self):
      assert self.rootHasNoParent()
      assert self.rootIsBlack()
      assert self.rootNodesHaveOnlyBlackChildren()
      assert self.treeIsAcyclic()
      assert self.parentsAreConsistent()
      return True

  def rootIsBlack(self):
      if self.parent is None:
          assert self.color == BLACK
          return True
\end{lstlisting}
You might (automatically) run these checks every time you add or remove from the red-black tree,
though an acylicity check might be expensive and best to disable in production.
This kind of check does work well with automatically-generated inputs from fuzzing,
which we'll discuss in a few weeks.

\section*{Test Doubles}
Mock objects are a particular kind of test double. We need test doubles
because objects collaborate with other objects, but we only want to test
one object at a time.
Meszaros categorizes test doubles as follows:
\begin{itemize}[noitemsep]
    \item dummy objects: these are not actually test doubles; they don't do anything, but just take up space in parameter lists. Are like {\tt null}, but get past nullness checks in code.
    \item fake objects: have actual behaviour (which is correct), but somehow unsuitable for use in production; typical example is an in-memory database.
    \item stubs: produce canned answers in response to interactions from the class under test.
    \item mocks: like stubs, also produce canned answers. Difference: mock objects also check that the class under test makes the appropriate calls.
    \item spies: usually wraps the real object (instead of the mock, which stubs it), and records interactions for later verification.
\end{itemize}
Shorter reference about test doubles: \url{martinfowler.com/articles/mocksArentStubs.html}

\section*{Mock Objects}
Before we talk about mock objects, let's look at a stub. Imagine that you have a service that
sends out emails. You don't actually want to send out emails while you're testing. So here's
a class that pretends to send out emails.
\codelisting{}{L03/MailServiceFake.java}
This stub permits \emph{behavioural verification}, as seen in the following assert in a test:
\begin{lstlisting}[language=Java]
  assertEquals(1, mailer.numberSent());
\end{lstlisting}
This is more like behavioural verification because it's checking that interactions that have happened. Technically, it's checking the state that records the interaction, but I'd still say behavioural rather than state. One could also check the recipients, contents of messages, etc.

\paragraph{jMock example.} 
Instead of state verification, we can also do behaviour verification. This is
jMock syntax.
\codelisting{}{L02/OrderInteractionTester.java}
The calls to {\tt mock()} create mock objects which have the
appropriate type.  If you are using the objects as simple dummy
objects, calling {\tt mock()} and {\tt proxy()} is enough. Note that
we have a real {\tt Order} object but we're giving it the fake proxy
objects, as created by the {\tt Mock}'s {\tt proxy()} methods.

We also specify the expected behaviour of the {\tt mailer} and the
{\tt warehouse}. The test case is saying that the mailer ought to have
{\tt send()} called on it once, and that the warehouse ought to have
{\tt hasInventory()} called; that method should return {\tt false()}.

\paragraph{EasyMock example.} Different mock object libraries have different syntax. Here's 
another example, this time for EasyMock.

\codelisting{}{L02/ExampleTest.java}
Here we are testing the {\tt ClassUnderTest} and creating a mock object
of {\tt Collaborator} type. EasyMock 2.3 reads the {\tt @Mock} annotation
and automatically fills in a mock object of the appropriate type.
In our test case, we call {\tt replay(mock)} to indicate that we are no
longer recording expectations, but are instead starting the test case itself.
In the above code, there are currently no expectations.

Let's add some expectations.
\codelisting{}{L02/TestAddDocument.java}
Here we record the fact that the mock should be called with {\tt documentAdded}
and a parameter "New Document". We also record that the mock's 
{\tt voteForRemoval} method should be called, and when that happens, it should return
value 42.
Finally, we add a call {\tt verify()} to let EasyMock
know that we're done and that it can go ahead and check that the expected behaviour actually happened.

\section*{Flaky Tests}
The second test engineering topic I want to talk about today is flaky tests. 
Flaky tests are those that sometimes fail (nondeterministically).
Flakiness is not something you want in your test cases. 
(I have heard one defense of a flaky test: it lets you know that 
the system has the potential to actually work.) 
In general, flaky tests don't play well with the expectation that
your test suite passes 100\%.

Reference:\\
Qingzhou Luo, Farah Hariri, Lamyaa Eloussi, Darko Marinov. ``An Empirical Analysis of Flaky Tests''. In Proceedings of Foundations of Software Engineering '14.

\paragraph{Dealing with flaky tests.} Companies with large test suites have found mitigations
for the flaky test suite problem. One can label known-flaky tests as flaky and automatically
re-run them to see if they eventually pass. One can also ignore or remove flaky tests.
But this is unsatisfactory: it takes a long time to re-run failing tests.

\paragraph{Causes of flakiness.} Luo et al studied 201 fixes to flaky tests in open-source
projects. They found that the three most common causes of fixable flaky tests were:
\begin{enumerate}[noitemsep]
    \item improper waits for asynchronous responses;
    \item concurrency; and
    \item test order dependency.
\end{enumerate}

The problem that caused flakiness for asynchronous waits was that there was typically a
{\tt sleep()} call which didn't wait long enough for the action (perhaps a network call)
to finish. The best practice is to use some sort of {\tt wait()} call to wait for the result
instead of hardcoding a sleep time.

Concurrency problems were what one might expect. The problem could either be in the system under
test or in the test itself. Problems included data races, atomicity violations, and deadlocks;
the solutions were the proper use of concurrency primitives (e.g. locks) as seen in CS343.

Test order dependency problems arose when some tests expected other tests to have already
executed (and left a side effect like a file in the filesystem). They came up especially 
in the transition from Java 6 to Java 7 because that transition changed the (not-guaranteed)
test execution order. The solution is to remove the dependency.

\section*{Not unit tests}
Many interesting things happen when different units interact.
Integration tests verify end-to-end functionality. They're slower, flakier, and harder both
to write and use. Focus on unit tests while developing code. But you eventually need
integration tests to make sure the user sees the right thing.

\section*{When to stop? Idea 1: Coverage}

So, in the testing space, we write a bunch of tests and hope it's good
enough. For most of us, it's never that fun to write tests. But, you
want to do a good job, so you should write some number of tests. What
is that number?

If you could test your function or program on every single input (and
if you had an oracle to tell you if the output was correct---more on that later), then that
would clearly be enough. But, this doesn't work. Even if your program is
not timing-sensitive. There are just too many inputs. 
Exponential growth strikes again.

Short of that, one metric that people use in industry is the notion of
code coverage. In particular, statement coverage and branch coverage.
There are other notions of coverage, but they are not widely used and
I don't think they actually tell you anything useful.

You could evaluate statement coverage and branch coverage based on
program source code and lines of code. When we talk about the real world, we'll do that. The easiest notation to reason about is an intermediate representation with a control-flow graph, but we don't actually need that in this version of this course. We will assume that each non-blank, non-comment line of code corresponds to one program statement.

\paragraph{Aside: white-box and black-box testing.} I don't think this
is really a big deal these days, but there is the term \emph{white-box} testing,
which means that you can look at the source code when you write tests,
and \emph{black-box} testing, where you can't.

\subsection*{Statement \& Branch Coverage}

I used to give a more formal definition, but it boils down to this.
You have a test suite and a program. 

Instrument the program to count whether each statement is executed or not.  Also instrument it to count whether each
branch is taken or not.

\emph{Statement coverage} is the fraction of statements that are
executed by the test suite. \emph{Branch coverage} is the fraction of
branches that are executed.

Let's look at an example, which is also available in the \texttt{code/L03} directory of the pdfs
repository. Here is some code.

\lstinputlisting{code/L03/foo.py}

And here's a test suite.

\lstinputlisting{code/L03/test_suite.py}

The textual report has much of the important information, though the HTML report is easier to navigate:

{\tiny
\begin{verbatim}
Name                                                         Stmts   Miss Branch BrPart  Cover   Missing
--------------------------------------------------------------------------------------------------------
/usr/lib/python3/dist-packages/_distutils_hack/__init__.py     101     96     38      0     6%   2-101, 111-239
L03/foo.py                                                      11      2      8      2    79%   4, 11
L03/test_suite.py                                               12      0      0      0   100%
--------------------------------------------------------------------------------------------------------
TOTAL                                                          124     98     46      2    21%
\end{verbatim}
}

One can add missing test cases to make sure that all the lines are visited. One can also observe
that, even with 100\% branch coverage, one is missing an important behaviour: what if \texttt{b} is 0?

\subsection*{Tracking Python Coverage in Python}
I said ``instrument the program''. Different languages have different support for this. In C, you need to compile the program with instrumentation. In Java, the virtual machine can collect information as the program executes (more useful if there is debug information). Python also provides hooks\footnote{We are following \url{https://www.fuzzingbook.org/html/Coverage.html} here.} so that you can collect information about program execution and implement your own \emph{dynamic program analyses}. 

Specifically, you can use the Python function \texttt{sys.settrace(f)} to register \texttt{f()} as a \emph{tracing function} which gets called once per line executed. It gets passed information about the line which is being executed and the current context (e.g. contents of variables and the call stack.)

Here is an example tracing function, which I've put in the repo at \texttt{code/L03/tracing.py}.
\begin{lstlisting}[language=Python]
from types import FrameType, TracebackType
coverage = []

def traceit(frame: FrameType, event: str, arg: Any) -> Optional[Callable]:
    """Trace program execution. To be passed to sys.settrace()."""
    if event == 'line':
        global coverage
        function_name = frame.f_code.co_name
        lineno = frame.f_lineno
        coverage.append(lineno)

    return traceit
\end{lstlisting}
This records the line number and function name to be executed in the global \texttt{coverage} list. More generally, the \texttt{frame} parameter includes information about the function name, line number, and local variables and arguments. The \texttt{event} parameter tells you that Python is about to execute a new \texttt{"line"}, \texttt{"call"}, or perform some other event. Finally, \texttt{arg} gives you additional information about the \texttt{event} when appropriate, e.g. the value being returned for a \texttt{"return"} event.

I've included a function \texttt{cgi\_decode()} from the \emph{Fuzzing Book}
in \texttt{tracing.py}, and we can use it to test out tracing:

\begin{lstlisting}[language=Python]
def cgi_decode_traced(s: str) -> None:
    global coverage
    coverage = []
    sys.settrace(traceit)  # tracing on
    cgi_decode(s)
    sys.settrace(None)     # tracing off
\end{lstlisting}

and if we call it and print out the coverage, we get something like:
\begin{lstlisting}[language=Python]
>>> cgi_decode_traced("a+b")
>>> print(coverage)
[12, 13, 12, 13, 12, 13, 12, 13, 12, 13, 12, 14, 12, 14, 12, 14, 12, 14, 12, 14, 12, 15, 12, 15, 12, 15, 12, 15, 12, 15, 12, 15, 12, 16, 12, 16, 12, 19, 20, 21, 22, 23, 25, 34, 35, 21, 22, 23, 24, 35, 21, 22, 23, 25, 34, 35, 21, 36]
\end{lstlisting}
The \emph{Fuzzing Book} continues with using Python introspection to retrieve the source code and pretty-print it again, highlighting which lines have and, importantly, have not been covered. This allows the user to write tests to ensure better statement coverage.

There is one more thing. The \emph{Fuzzing Book} also talks about the usage of \texttt{with} to turn tracing on and off, and storing the result to a specified variable:
\begin{lstlisting}[language=Python]
with Coverage() as cov:
    function_to_be_traced()
c = cov.coverage()
\end{lstlisting}
Here, the \texttt{Coverage} class does a bit more magic to provide the desired functionality, but we won't talk about it. Basically, we can print out either \texttt{cov} (which has a useful pretty-printing function) or the result of its \texttt{coverage()} method, which is a set of method/line pairs.

\subsection*{Infeasible Test Requirements; How Much Coverage, Anyway?}

About ``better statement coverage''. For toy programs, we can reach 100\% statement and branch coverage.
For real programs, this is still theoretically possible, but becomes
impractical. 

We'll wrap up our unit on defining test suites by exploring the question
``How much is enough?'' We'll discuss coverage first and then mutation testing
as ways of answering this question.

First, we can look at actual test suites and see how much coverage they achieve.
I collected this data a few years ago, measured with the EclEmma tool.

\begin{center}
  \includegraphics[height=2in]{L03/coverage.png}
\end{center}

We can see that the coverage varies between 20\% and 95\% on actual
open-source projects. I investigated further and found that while Weka has low
test coverage, it instead uses scientific peer review for QA: its features
come from published articles. Common practice in industry is that about 80\%
coverage (doesn't matter which kind) is good enough.

Let's look at a more specific case study, JUnit. The rest of this lecture is
based on a blog post by Arie van Deursen:

\begin{center}
  \url{https://avandeursen.com/2012/12/21/line-coverage-lessons-from-junit/}
\end{center}

Although you might think of JUnit as something that just magically exists in the world,
it is a software artifact too. JUnit is written by developers who obviously really care
about testing. Let's see what they do.

Here's the Cobertura report for JUnit:

\begin{center}
  \includegraphics[height=2in]{L03/cobertura-junit.png}
\end{center}

\paragraph{Stats.} Overall instruction (statement) coverage for JUnit 4.11 is about 85\%; there are
13,000 lines of code and 15,000 lines of test code. (It's not that
unusual for there to be more tests than code.) This is consistent with
the industry average.

\paragraph{Deprecated code?} Sometimes library authors decide that some functionality
was not a good idea after all. In that case they might \emph{deprecate} some methods or
classes, signalling that these APIs will disappear in the future.

In JUnit, deprecated and older code has lower coverage levels. Its 13 deprecated
classes have only 65\% instruction coverage. Ignoring deprecated code, JUnit achieves
93\% instruction coverage. Furthermore, newer code in
package {\tt org.junit.*} has 90\% instruction coverage, while older code in
{\tt junit.*} has 70\% instruction coverage.

(Why is this? Perhaps the coverage decreased over time for the deprecated code, since
no one is really maintaining it anymore, and failing test cases just get removed.)

\paragraph{Untested class.} The blog post points out one class that was completely
untested, which is unusual for JUnit. It turns out that the code came with tests,
but that the tests never got run because they were never added to any test suites.
Furthermore, these tests also failed, perhaps because no one had ever tried them.
The continuous integration infrastructure did not detect this change. (More on CI
later.)

\paragraph{What else?} Arie van Deursen characterizes the remaining 6\% as
``the usual suspects''. In JUnit's case, there was no method with more than
2 to 3 uncovered lines. Here's what he found.

\noindent
\emph{Too simple to test.} Sometimes it doesn't make sense to test a method, because
it's not really doing anything. For instance:
\begin{lstlisting}[language=Java]
  public static void assumeFalse(boolean b) {
    assumeTrue(!b);
  }
\end{lstlisting}
or just getters or {\tt toString()} methods (which can still be wrong).

The empty method is also too simple to test; one might write such a method to allow
it to be overridden in subclasses:
\begin{lstlisting}[language=Java]
  /**
  * Override to set up your specific external resource.
  *
  * @throws if setup fails (which will disable {@code after}
  */
  protected void before() throws Throwable {
    // do nothing
  }
\end{lstlisting}

\noindent \emph{Dead by design.} Sometimes a method really should never be called,
for instance a constructor on a class that should never be instantiated:
\begin{lstlisting}[language=Java]
  /**
  * Protect constructor since it is a static only class
  */
  protected Assert() { }
\end{lstlisting}

A related case is code that should never be executed:
\begin{lstlisting}[language=Java]
  catch (InitializationError e) {
    throw new RuntimeException(
    "Bug in saff's brain: " +
    "Suite constructor, called as above, should always complete");
  }
\end{lstlisting}
Similarly, switch statements may have unreachable default cases. Or other unreachable code.
Sometimes the code is just highly unlikely to happen:
\begin{lstlisting}[language=Java]
  try {
    ...
  } catch (InitializationError e) {
    return new ErrorReportingRunner(null, e); // uncovered
  }
\end{lstlisting}

\paragraph*{Conclusions.} We explored empirically the instruction coverage of JUnit,
which is written by people who really care about testing. Don't forget
that coverage doesn't actually guarantee, by itself, that your code is
well-exercised; what is in the tests matters too. For non-deprecated
code, they achieved 93\% instruction coverage, and so it really is
possible to have no more than 2-3 untested lines of code per
method. It's probably OK to have lower coverage for deprecated
code. Beware when you are adding a class and check that you are also
testing it.




\end{document}
